---
title: "Pollution"
author: "Asal"
date: "2/20/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r}
install.packages("dplyr")                                         # Install dplyr package
install.packages("plyr")                                          # Install plyr package
install.packages("readr")                                         # Install readr package
install.packages("validate")                                      # Install dplyr package
install.packages("ggplot2")
install.packages("scales")
install.packages("grid")
install.packages("corrplot")
install.packages("xlsx")



library("dplyr")                                                  # Load dplyr package
library("plyr")                                                   # Load plyr package
library("readr/")                                                  # Load readr package
library(validate)                                                 # Load validate package
library(corrplot)
library(ggplot2)
library(corrplot)
library(xlsx)
```


# 1. Organise and clean the data

# 1.1 Unify all files into a single file
```{r}
# Identify all csv files in folder
pollution_df <- list.files(path = "F:/Documents/MSc Data Science & Analytics/Distributed Data Analysis/Datasets/Air Pollution", pattern = "*.csv", full.names = TRUE) %>% 
  
# Store all files in list  
  lapply(read_csv) %>%   
  
# Combine data sets into one data set
  bind_rows                                                       

# Print data to RStudio console                                                        
pollution_df

# Get the summary and structure of the dataset to have an overview of data types, number of missing values, minimum and maximum amount of each variable
str(pollution_df)
summary(pollution_df)

apply(X = is.na(pollution_df2), MARGIN = 2, FUN = sum)
sum(is.na(pollution_df2))
colMeans(is.na(pollution_df2))
mean(is.na(pollution_df2))
```


# 1.2 Check quality of data 

# 1.2.1 Visual analysis of data
```{r}
# Create boxplot of all variables and show all five plots regarding air pollutants in one page. Do the same for weather conditions
# Air pollutants
opar <- par(no.readonly = TRUE)
par(mfrow = c(2,3))

boxplot(pollution_df$PM10,
        main = "PM10")

boxplot(pollution_df$SO2,
        main = "SO2")

boxplot(pollution_df$NO2,
        main = "NO2")

boxplot(pollution_df$CO,
        main = "CO")

boxplot(pollution_df$O3,
        main = "O3")

par(opar)

# Weather conditions
opar <- par(no.readonly = TRUE)
par(mfrow = c(2,3))

boxplot(pollution_df$TEMP,
        main = "Temperature")

boxplot(pollution_df$PRES,
        main = "Pressure")

boxplot(pollution_df$DEWP,
        main = "Dew Point")

boxplot(pollution_df$RAIN,
        main = "Rainfall")

boxplot(pollution_df$WSPM,
        main = "Wind Speed")

par(opar)

# Checking Corrolation

#first of all, the variables that are not contributing to pollution and are not numerical should be dropped from the dataset.
num_pollution_data <- subset(pollution_df, select =  -c(No, year, month, day, hour, wd, station))

#Secondly, the corrolation between all variables are extracted with the help of cor function.
cor(na.omit(num_pollution_data))

#Thirdly, the matrix of the correlation table is created with cor.mat function and is visualized by corrplot function in the following. White cells indicate that there are not any relationship between two variables. Red represents a reverse and blue indicates a direct correlation between the two variables. 
cor.mat <- cor(na.omit(num_pollution_data))
corrplot(cor.mat,diag=F,type="upper",insig = "p-value",number.digits = 1,addCoef.col = 'black',tl.cex=0.8)
```


# 1.2.2 Pre-processing Data
```{r}
# Dropping PM2.5 due to high correlation
pollution_df2 <- subset(pollution_df, select = -c(PM2.5))

pollution_df2$wd <- as.factor(pollution_df2$wd)
levels(pollution_df2$wd)


# Each Station has its own NO starting from 1. Therefore, No column is not unique in the unified dataframe.
is_unique(pollution_df2$No)
pollution_df3= subset(pollution_df2, select = -c(No) )
pollution_df3["No"] <- seq(1,420768)
pollution_df4 <- pollution_df3 %>% relocate(No, .before = year)

str(pollution_df4)


# Export pollution_df2 (cleaned data) as .csv file in a specific location in the PC to apply imputation on the missing value in Dask
write.csv(pollution_df4,"F:/Documents/MSc Data Science & Analytics/Distributed Data Analysis/Coding/Pollution Dataset.csv", row.names = TRUE)
```

# 2. Exploratory Data Analysis (EDA)

## 2.1 Quality Checking

```{r}
# Reading the Cleaned dataset which is extracted from Dask
cleaned_df <- read.csv('F:/Documents/MSc Data Science & Analytics/Distributed Data Analysis/Coding/Final Cleaned Pollution.csv')
summary(cleaned_df)

# Define rules for air pollutants and weather conditions 
pollution_rules <- validator(
  range_year = in_range(year,2013,2017),
  range_month = in_range(month,1,12),
  range_day = in_range(day,1,31),
  range_hour = in_range(hour,0,23),
  range_PM10 = in_range(PM10,0,300),
  rnage_SO2 = in_range(SO2,0,150),
  range_NO2 = in_range(NO2,0,130),
  range_CO = in_range(CO,0,3500),
  range_O3 = in_range(O3,0,300)
)

# Check quality of data by assessing the defined rules. Then get the summary and plot for better evaluation
checking_quality <- confront(cleaned_df, pollution_rules)
summary(checking_quality)
plot(checking_quality, xlab = "")

#first of all, the target variable, the variables that are not contributing to pollution and are not numerical should be dropped from the dataset.
num_pollution_data <- subset(cleaned_df, select =  -c(No, year, month, day, hour, wd, station, PM10))
```

# 2.2 Applying PCA 
```{r}
# perform PCA on the dataset

pc_pollution <- prcomp(num_pollution_data, center = T, scale = T)

# inspect the attributes of the PCA object returned by prcomp
attributes(pc_pollution)

summary(pc_pollution)
 
print(pc_pollution)
```   

# 2.3 Visual analysis of PCA results
```{r}
# plot the variance per PC
plot(pc_pollution)


# calculate the proportion of explained variance (PEV) from the std values
pc_pollution_var <- pc_pollution$sdev^2
pc_pollution_var
pc_pollution_PEV <- pc_pollution_var / sum(pc_pollution_var)
pc_pollution_PEV


# plot the cumulative value of PEV for increasing number of additional PCs
opar <- par(no.readonly = TRUE)
plot(
  cumsum(pc_pollution_PEV),
  ylim = c(0,1),
  xlab = 'PC',
  ylab = 'cumulative PEV',
  pch = 20,
  col = 'orange'
)
abline(h = 0.8, col = 'red', lty = 'dashed')
par(opar)

# compute total variance
variance = pc_pollution_var / sum(pc_pollution_var)
 
# Scree plot
qplot(c(1:4), variance) +
  geom_line() +
  geom_point(size=4)+
  xlab("Principal Component") +
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)

# Line Scree plot
var_explained = pc_pollution$sdev^2 / sum(pc_pollution$sdev^2)

library(ggplot2)

qplot(c(1:9), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 0.4)

# get and inspect the loadings for each PC
pc_pollution_loadings <- pc_pollution$rotation
pc_pollution_loadings

# plot the loadings for the first three PCs as a barplot
opar <- par(no.readonly = TRUE)
colvector = c('mistyrose3', 'lightsteelblue3', 'lightyellow', 'olivedrab', 'darkgoldenrod3', 'lightsteelblue1', 'aquamarine4', 'bisque4', 'lightpink3', 'azure4')
labvector = c('PC1', 'PC2', 'PC3')
barplot(
  pc_pollution_loadings[,c(1:3)],
  beside = T,
    yaxt = 'n',
  names.arg = labvector,
  col = colvector,
  ylim = c(-1,1),
  border = 'GRAY',
  ylab = 'loadings'
)
axis(2, seq(-1,1,0.1))
  legend(
  'bottomright',
  bty = 'n',
  col = colvector,
  pch = 15,
  row.names(pc_pollution_loadings)
)

par(opar)
```


# Creating dataset with PCs
```{r}
pc_pollution_x <- data.frame(pc_pollution$x)
names(pc_pollution_x)
pollution_new_dataset <- subset(pc_pollution_x, select = -c(PC5, PC6, PC7, PC8, PC9))
head(pollution_new_dataset)

# Add target variable to the new dataset
PM10 <- cleaned_df$PM10
pollution_new_dataset$PM10 <- PM10
head(pollution_new_dataset)

# Export created dataset as .csv file to apply machine learning in Dask
write.csv(pollution_new_dataset,"F:/Documents/MSc Data Science & Analytics/Distributed Data Analysis/Coding/Reduced Pollution Dataset.csv", row.names = TRUE)
```


# Visualizing RMSE of Neural Network which is derived from Dask
```{r}
library(ggplot2)
library(ggrepel)

rmse <- read.csv("RMSE.csv")
print(rmse)

ggplot(rmse, aes(Hidden.layer , RMSE, label = Architecture)) + geom_point() +  geom_label_repel()
```

